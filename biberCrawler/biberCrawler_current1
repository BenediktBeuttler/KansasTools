from ast import If
from fileinput import filename
from bs4 import BeautifulSoup
import requests
import codecs
import re
import uuid

# Variables which have the same value for all articles
basePath = "biberCrawler"
metaFileName = basePath + "/bibermeta.csv"
logs = basePath + "/logs.txt"
biberLizenz = 'unknown'
encoding = "utf-8"
author = 'dasBiber'
org = 'dasBiber'# 
orgLink = 'https://www.dasbiber.at/'
processed_files = basePath +  "processed-biber-files.txt"


def safe_text_in_file(meta_title, text):
     clean_fileName  = re.sub('[^a-zA-Z0-9 \n\.]', " ", meta_title)
     print("After replacing special characters with spaces: " + clean_fileName)
     fileName = basePath+ "/txts/" + clean_fileName.replace(" ", "") + '.txt'

     with codecs.open(fileName, 'a', encoding) as f:
        f.write(text)
        f.close()



def crawl_page_meta_and_text(url_link):
     
     
     # get html of website
     r = requests.get(url_link)
     data = r.text
     soup = BeautifulSoup(data, features="html.parser")
     
     # search meta data 
     meta_title = soup.title
     meta_title = meta_title.string 
     meta_title = meta_title[:-11] 
     url = (soup.find('link', rel=re.compile('canonical'))['href']) 
     
    # add results to metacsv
    # addToMetaFile(fileName, url, meta_title, author, org, orgLink)

    # find text
     text = ""
     parent = soup.find("div", {"class":"node-content"})
     
     contentChildren = parent.findChildren()
     for child in contentChildren:   
        
        if (child.name != "div" ): 
            txt = child.get_text().strip()
            if ((txt != "&nbsp") and (txt !="*BEZAHLTE ANZEIGE*")):
                text += txt   
            else: 
                continue
     
           
    # safe text in for it created file
     safe_text_in_file(meta_title, text)


def Crawl():
    basePageURL = "https://www.dasbiber.at/articles"
    for i in range(0,2):
        if i == 0:
            get_each_URL1(basePageURL)
        else:
            
            newPageURL = basePageURL + "?page=" +str(i)

            get_each_URL1(newPageURL)


def get_each_URL1(newPageURL ):
    baseURL = "https://www.dasbiber.at//"
    r = requests.get(newPageURL)
    data = r.text
    soup = BeautifulSoup(data, features="html.parser")
    

    heading_parent1 = soup.find("div", {"class": "main-group group-cols-1 group-6 grid grid-6"})
    children = heading_parent1.findChildren()
    #print(len(children))
    i= 0
    for child in children:
        if (child.name == "div" and "field-content" in child.get("class", [])):
            tags_link = child.findChildren()
            tag_url = tags_link[0]
            final_url = baseURL + tag_url['href']
            print(final_url)
            # add url to logs file 
           # write_to_logs(str(final_url))
            # get meta data and  saved in csv
            # get text safed to file
            # safe crawled page in processed-biber-files
            crawl_page_meta_and_text(final_url)
            # get text



#crawl_txt("https://www.dasbiber.at/content/freispruch-warum-die-klage-gegen-sos-balkanroute-gesellschaftliche-folgen-hat")
#crawl_page_meta_and_text("https://www.dasbiber.at/content/die-welt-darf-keinen-zweiten-voelkermord-zulassen")
#get_each_URL1("https://www.dasbiber.at/articles")
Crawl()
