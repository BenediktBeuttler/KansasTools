from ast import If
from fileinput import filename
from bs4 import BeautifulSoup
import requests
import codecs
import re
import uuid

# Variables which have the same value for all articles
basePath = "biberCrawler"
metaFileName = basePath + "/bibermeta.csv"
logs = basePath + "/logs.txt"
biberLizenz = 'unknown'
encoding = "utf-8"
author = 'dasBiber'
org = 'dasBiber'# 
orgLink = 'https://www.dasbiber.at/'
processed_files = basePath +  "processed-biber-files.txt"
#logs = 'logs.txt'  # Define logs file path
# 



           
    #hand text over to function to save 

def crawl_page_meta_and_text(url_link):
     
     print(url_link)
     # get html of website
     r = requests.get(url_link)
     data = r.text
     soup = BeautifulSoup(data, features="html.parser")
     
     # search meta data 
     meta_title = soup.title
     meta_title = meta_title.string 
     meta_title = meta_title[:-11]
     print(meta_title)
     fileName = meta_title.replace(" ", "") + '.txt'
     print(fileName)
     url = (soup.find('link', rel=re.compile('canonical'))['href']) 
     
    # add results to metacsv
    # addToMetaFile(fileName, url, meta_title, author, org, orgLink)

    # find text
     text = ""
     parent = soup.find("div", {"class":"node-content"})
     print(type(parent))
    # print("parent is following type: " + len(parent))
    #print("result of parent"+parent)
     contentChildren = parent.findChildren()
    #print(len(contentChildren))
    #print(type(contentChildren))
     for child in contentChildren:   
        
        if (child.name != "div" ): 
            txt = child.get_text().strip()
            if ((txt != "&nbsp") and (txt !="*BEZAHLTE ANZEIGE*")):
                text += txt
                
                # def for saving text
              #  safe_text_in_file(fileName, text)
                
            else: 
                continue
           
    #hand text over to function to save 
     print(text)


def Crawl():
    basePageURL = "https://www.dasbiber.at/articles"
    for i in range(0,5):
        if i == 0:
            get_each_URL1(basePageURL)
        else:
            
            newPageURL = basePageURL + "?page=" +str(i)
            print("I reached the else statement"+newPageURL)
            get_each_URL1(newPageURL)


def get_each_URL1(newPageURL ):
    baseURL = "https://www.dasbiber.at//"
    r = requests.get(newPageURL)
    data = r.text
    soup = BeautifulSoup(data, features="html.parser")
    

    heading_parent1 = soup.find("div", {"class": "main-group group-cols-1 group-6 grid grid-6"})
    children = heading_parent1.findChildren()
    #print(len(children))
    i= 0
    for child in children:
        if (child.name == "div" and "field-content" in child.get("class", [])):
            tags_link = child.findChildren()
            tag_url = tags_link[0]
            final_url = baseURL + tag_url['href']
            print(final_url)
            # add url to logs file 
           # write_to_logs(str(final_url))
            # get meta data and  saved in csv
            # get text safed to file
            # safe crawled page in processed-biber-files
            crawl_page_meta_and_text(final_url)
            # get text



#crawl_txt("https://www.dasbiber.at/content/freispruch-warum-die-klage-gegen-sos-balkanroute-gesellschaftliche-folgen-hat")
#crawl_page_meta_and_text("https://www.dasbiber.at/content/die-welt-darf-keinen-zweiten-voelkermord-zulassen")
get_each_URL1("https://www.dasbiber.at/articles")

